{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter, medfilt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Dict, List, Tuple, Union, Optional\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "_XKSZlXoc8Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración para visualización\n",
        "# Check available styles\n",
        "print(plt.style.available)\n",
        "\n",
        "# Try using the style again. If the above printout shows a different name\n",
        "# like 'seaborn-v0_8-whitegrid', use that instead.\n",
        "try:\n",
        "    plt.style.use('seaborn-whitegrid')  # Keep the original name first\n",
        "except OSError:\n",
        "    # If the above fails, try the versioned name if it was listed in available\n",
        "    if 'seaborn-v0_8-whitegrid' in plt.style.available:\n",
        "        # Corrected typo: 'seabarn' changed to 'seaborn'\n",
        "        plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    else:\n",
        "        # As a fallback, you can also ensure seaborn is imported and then rely on it setting the style.\n",
        "        # However, directly using plt.style.use is preferred if the style name is known.\n",
        "        print(\"Could not find 'seaborn-whitegrid' or 'seaborn-v0_8-whitegrid' in available styles.\")\n",
        "        print(\"Ensure seaborn is correctly installed and imported.\")\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqnQvdnc-MW",
        "outputId": "8636b20d-f4d5-40b9-ef6f-59888a721cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir directorios de trabajo\n",
        "# Ajustar esta ruta a la ubicación de tus archivos en Drive\n",
        "BASE_DIR = '/content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video'\n",
        "\n",
        "# Directorios para datos y salida\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'dataset')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'processed_data')\n",
        "\n",
        "# Crear directorios si no existen\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Directorio base: {BASE_DIR}\")\n",
        "print(f\"Directorio de datos: {DATA_DIR}\")\n",
        "print(f\"Directorio de salida: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yicN6j4SdVHs",
        "outputId": "1b2edec5-3076-4997-cd63-7a851dc693e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directorio base: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video\n",
            "Directorio de datos: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset\n",
            "Directorio de salida: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones auxiliares para manipulación de datos\n",
        "def dataframe_to_landmark_dict(frame_data: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Convierte un DataFrame de landmarks a formato de diccionario.\"\"\"\n",
        "    landmarks_dict = {}\n",
        "    for _, row in frame_data.iterrows():\n",
        "        landmark_name = row['landmark_name']\n",
        "        coords = np.array([row['x'], row['y'], row['z'], row['visibility']])\n",
        "        landmarks_dict[landmark_name] = coords\n",
        "\n",
        "    return landmarks_dict\n",
        "\n",
        "def landmark_dict_to_dataframe(landmarks_dict: Dict[str, np.ndarray]) -> pd.DataFrame:\n",
        "    \"\"\"Convierte un diccionario de landmarks a formato DataFrame.\"\"\"\n",
        "    rows = []\n",
        "    for landmark_name, coords in landmarks_dict.items():\n",
        "        rows.append({\n",
        "            'landmark_name': landmark_name,\n",
        "            'x': coords[0],\n",
        "            'y': coords[1],\n",
        "            'z': coords[2],\n",
        "            'visibility': coords[3]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "67CjfYJ0dOpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para normalización de landmarks\n",
        "class LandmarkNormalizer:\n",
        "    \"\"\"\n",
        "    Clase para normalizar coordenadas de landmarks corporales obtenidos de MediaPipe/OpenPose.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reference_points: List[str] = None):\n",
        "        # Puntos de referencia por defecto para normalización\n",
        "        self.reference_points = reference_points or ['LEFT_HIP', 'RIGHT_HIP', 'LEFT_SHOULDER', 'RIGHT_SHOULDER']\n",
        "\n",
        "    def normalize_by_hip_center(self, landmarks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Normaliza todas las coordenadas respecto al centro de la cadera.\"\"\"\n",
        "        # Calcular el centro de la cadera (promedio entre cadera izquierda y derecha)\n",
        "        hip_center = (landmarks.get('LEFT_HIP', np.zeros(4)) + landmarks.get('RIGHT_HIP', np.zeros(4))) / 2\n",
        "\n",
        "        # Normalizar todos los landmarks restando el centro de la cadera\n",
        "        normalized_landmarks = {}\n",
        "        for landmark_name, coords in landmarks.items():\n",
        "            # Mantener la visibilidad sin cambios (componente 3)\n",
        "            normalized_coords = coords.copy()\n",
        "            normalized_coords[:3] = coords[:3] - hip_center[:3]\n",
        "            normalized_landmarks[landmark_name] = normalized_coords\n",
        "\n",
        "        return normalized_landmarks\n",
        "\n",
        "    def normalize_by_body_scale(self, landmarks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Normaliza las coordenadas dividiéndolas por una escala corporal de referencia.\"\"\"\n",
        "        # Calcular los centros\n",
        "        shoulder_center = (landmarks.get('LEFT_SHOULDER', np.zeros(4)) +\n",
        "                          landmarks.get('RIGHT_SHOULDER', np.zeros(4))) / 2\n",
        "        hip_center = (landmarks.get('LEFT_HIP', np.zeros(4)) +\n",
        "                     landmarks.get('RIGHT_HIP', np.zeros(4))) / 2\n",
        "\n",
        "        # Calcular la distancia torso como factor de escala\n",
        "        torso_length = np.linalg.norm(shoulder_center[:3] - hip_center[:3])\n",
        "\n",
        "        # Evitar división por cero\n",
        "        scale_factor = torso_length if torso_length > 0 else 1.0\n",
        "\n",
        "        # Normalizar por escala\n",
        "        normalized_landmarks = {}\n",
        "        for landmark_name, coords in landmarks.items():\n",
        "            normalized_coords = coords.copy()\n",
        "            normalized_coords[:3] = coords[:3] / scale_factor\n",
        "            normalized_landmarks[landmark_name] = normalized_coords\n",
        "\n",
        "        return normalized_landmarks\n",
        "\n",
        "    def normalize_by_orientation(self, landmarks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Normaliza la orientación rotando el cuerpo.\"\"\"\n",
        "        # Obtener vector entre hombros (eje de referencia horizontal)\n",
        "        left_shoulder = landmarks.get('LEFT_SHOULDER', np.zeros(4))\n",
        "        right_shoulder = landmarks.get('RIGHT_SHOULDER', np.zeros(4))\n",
        "\n",
        "        shoulder_vector = right_shoulder[:3] - left_shoulder[:3]\n",
        "\n",
        "        # Si no hay información de hombros, no rotar\n",
        "        if np.all(shoulder_vector == 0):\n",
        "            return landmarks\n",
        "\n",
        "        # Calcular ángulo con respecto al eje X\n",
        "        target_vector = np.array([1, 0, 0])\n",
        "        shoulder_vector_norm = shoulder_vector / np.linalg.norm(shoulder_vector)\n",
        "\n",
        "        # Calcular ángulo entre vectores en el plano XY\n",
        "        angle = np.arccos(np.clip(np.dot(shoulder_vector_norm[:2], target_vector[:2]), -1.0, 1.0))\n",
        "\n",
        "        # Determinar dirección de rotación\n",
        "        if shoulder_vector_norm[1] < 0:\n",
        "            angle = -angle\n",
        "\n",
        "        # Matriz de rotación 2D (en el plano XY)\n",
        "        cos_angle, sin_angle = np.cos(angle), np.sin(angle)\n",
        "        rotation_matrix = np.array([\n",
        "            [cos_angle, -sin_angle, 0],\n",
        "            [sin_angle, cos_angle, 0],\n",
        "            [0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        # Aplicar rotación a todos los landmarks\n",
        "        normalized_landmarks = {}\n",
        "        for landmark_name, coords in landmarks.items():\n",
        "            rotated_coords = coords.copy()\n",
        "            rotated_coords[:3] = rotation_matrix @ coords[:3]\n",
        "            normalized_landmarks[landmark_name] = rotated_coords\n",
        "\n",
        "        return normalized_landmarks\n",
        "\n",
        "    def normalize_all(self, landmarks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Aplica todos los métodos de normalización en secuencia.\"\"\"\n",
        "        # Aplicar normalización secuencial\n",
        "        normalized = self.normalize_by_hip_center(landmarks)\n",
        "        normalized = self.normalize_by_body_scale(normalized)\n",
        "        normalized = self.normalize_by_orientation(normalized)\n",
        "\n",
        "        return normalized\n",
        "\n",
        "    def preprocess_landmarks_frame(self, frame_data: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Convierte datos de landmarks de formato DataFrame a diccionario y normaliza.\"\"\"\n",
        "        # Convertir DataFrame a formato de diccionario\n",
        "        landmarks_dict = {}\n",
        "        for _, row in frame_data.iterrows():\n",
        "            landmark_name = row['landmark_name']\n",
        "            coords = np.array([row['x'], row['y'], row['z'], row['visibility']])\n",
        "            landmarks_dict[landmark_name] = coords\n",
        "\n",
        "        # Aplicar normalización completa\n",
        "        return self.normalize_all(landmarks_dict)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_dataframe(normalized_landmarks: Dict[str, np.ndarray]) -> pd.DataFrame:\n",
        "        \"\"\"Convierte landmarks normalizados de diccionario a DataFrame.\"\"\"\n",
        "        rows = []\n",
        "        for landmark_name, coords in normalized_landmarks.items():\n",
        "            rows.append({\n",
        "                'landmark_name': landmark_name,\n",
        "                'x': coords[0],\n",
        "                'y': coords[1],\n",
        "                'z': coords[2],\n",
        "                'visibility': coords[3]\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "# Función para normalizar una secuencia de frames\n",
        "def normalize_landmark_sequence(landmark_frames: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
        "    \"\"\"Normaliza una secuencia temporal de frames de landmarks.\"\"\"\n",
        "    normalizer = LandmarkNormalizer()\n",
        "    normalized_frames = []\n",
        "\n",
        "    for frame in landmark_frames:\n",
        "        landmarks_dict = normalizer.preprocess_landmarks_frame(frame)\n",
        "        normalized_frame = LandmarkNormalizer.convert_to_dataframe(landmarks_dict)\n",
        "        normalized_frames.append(normalized_frame)\n",
        "\n",
        "    return normalized_frames"
      ],
      "metadata": {
        "id": "maB55pvkdY5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para filtrado de ruido\n",
        "class LandmarkFilter:\n",
        "    \"\"\"\n",
        "    Clase para aplicar filtros a las coordenadas de landmarks y reducir ruido.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size: int = 5):\n",
        "        self.window_size = window_size\n",
        "        # Almacenamiento para filtros que requieren estado (Kalman)\n",
        "        self.prev_landmarks = None\n",
        "        self.kalman_states = {}\n",
        "        self.kalman_covs = {}\n",
        "\n",
        "    def moving_average(self, landmark_sequence: List[Dict[str, np.ndarray]]) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Aplica filtro de media móvil a una secuencia de landmarks.\"\"\"\n",
        "        if len(landmark_sequence) < self.window_size:\n",
        "            return landmark_sequence\n",
        "\n",
        "        filtered_sequence = []\n",
        "\n",
        "        # Mantener los primeros frames sin modificar (mitad de la ventana)\n",
        "        half_window = self.window_size // 2\n",
        "        filtered_sequence.extend(landmark_sequence[:half_window])\n",
        "\n",
        "        # Aplicar media móvil al centro de la secuencia\n",
        "        for i in range(half_window, len(landmark_sequence) - half_window):\n",
        "            filtered_frame = {}\n",
        "\n",
        "            # Para cada landmark, promediar sobre la ventana\n",
        "            for landmark_name in landmark_sequence[i].keys():\n",
        "                avg_coords = np.zeros(4)\n",
        "                count = 0\n",
        "\n",
        "                # Calcular promedio dentro de la ventana\n",
        "                for j in range(i - half_window, i + half_window + 1):\n",
        "                    if landmark_name in landmark_sequence[j]:\n",
        "                        avg_coords += landmark_sequence[j][landmark_name]\n",
        "                        count += 1\n",
        "\n",
        "                if count > 0:\n",
        "                    avg_coords /= count\n",
        "                    filtered_frame[landmark_name] = avg_coords\n",
        "\n",
        "            filtered_sequence.append(filtered_frame)\n",
        "\n",
        "        # Mantener los últimos frames sin modificar\n",
        "        filtered_sequence.extend(landmark_sequence[-half_window:])\n",
        "\n",
        "        return filtered_sequence\n",
        "\n",
        "    # Implementaciones simples para los métodos faltantes\n",
        "    def median_filter(self, landmark_sequence: List[Dict[str, np.ndarray]]) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Aplica filtro de mediana a cada trayectoria de landmark.\"\"\"\n",
        "        # Implementación simplificada pero funcional\n",
        "        if len(landmark_sequence) < 3:  # Necesitamos al menos 3 frames para filtrar\n",
        "            return landmark_sequence\n",
        "\n",
        "        filtered_sequence = landmark_sequence.copy()\n",
        "\n",
        "        # Procesar cada frame (excepto los extremos)\n",
        "        for i in range(1, len(landmark_sequence) - 1):\n",
        "            filtered_frame = {}\n",
        "\n",
        "            # Para cada landmark en el frame actual\n",
        "            for landmark_name in landmark_sequence[i].keys():\n",
        "                coords_list = []\n",
        "\n",
        "                # Recopilar coordenadas de frames adyacentes\n",
        "                for j in range(i-1, i+2):  # [i-1, i, i+1]\n",
        "                    if landmark_name in landmark_sequence[j]:\n",
        "                        coords_list.append(landmark_sequence[j][landmark_name])\n",
        "\n",
        "                # Si hay suficientes puntos para calcular la mediana\n",
        "                if len(coords_list) >= 2:\n",
        "                    # Calcular mediana por dimensión\n",
        "                    coords_array = np.array(coords_list)\n",
        "                    median_coords = np.median(coords_array, axis=0)\n",
        "                    filtered_frame[landmark_name] = median_coords\n",
        "                else:\n",
        "                    # Mantener valor original si no hay suficientes datos\n",
        "                    filtered_frame[landmark_name] = landmark_sequence[i][landmark_name]\n",
        "\n",
        "            filtered_sequence[i] = filtered_frame\n",
        "\n",
        "        return filtered_sequence\n",
        "\n",
        "    def simple_kalman_filter(self, landmark_sequence: List[Dict[str, np.ndarray]]) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Implementa un filtro de Kalman simple para landmarks.\"\"\"\n",
        "        # Implementación muy simplificada para suavizar sin la complejidad de Kalman\n",
        "        if len(landmark_sequence) < 3:\n",
        "            return landmark_sequence\n",
        "\n",
        "        # Usar promedio ponderado para simular Kalman\n",
        "        filtered_sequence = []\n",
        "        filtered_sequence.append(landmark_sequence[0])  # Primer frame sin modificar\n",
        "\n",
        "        # Factor de ponderación (simula ganancia de Kalman)\n",
        "        alpha = 0.7  # Peso para el valor predicho\n",
        "\n",
        "        for i in range(1, len(landmark_sequence)):\n",
        "            prev_frame = filtered_sequence[i-1]\n",
        "            curr_frame = landmark_sequence[i]\n",
        "\n",
        "            filtered_frame = {}\n",
        "\n",
        "            # Para cada landmark en el frame actual\n",
        "            for landmark_name in curr_frame.keys():\n",
        "                if landmark_name in prev_frame:\n",
        "                    # Aplicar filtro tipo Kalman simplificado (promedio ponderado)\n",
        "                    filtered_coords = alpha * curr_frame[landmark_name] + (1-alpha) * prev_frame[landmark_name]\n",
        "                    filtered_frame[landmark_name] = filtered_coords\n",
        "                else:\n",
        "                    # Si no hay valor previo, usar actual\n",
        "                    filtered_frame[landmark_name] = curr_frame[landmark_name]\n",
        "\n",
        "            # Añadir landmarks que estaban en el frame anterior pero no en el actual\n",
        "            for landmark_name in prev_frame.keys():\n",
        "                if landmark_name not in curr_frame:\n",
        "                    filtered_frame[landmark_name] = prev_frame[landmark_name]\n",
        "\n",
        "            filtered_sequence.append(filtered_frame)\n",
        "\n",
        "        return filtered_sequence\n",
        "\n",
        "    def interpolate_missing_landmarks(self, landmark_sequence: List[Dict[str, np.ndarray]]) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Interpola landmarks faltantes o de baja confianza en la secuencia.\"\"\"\n",
        "        # Implementación simplificada para interpolar\n",
        "        if len(landmark_sequence) < 3:\n",
        "            return landmark_sequence\n",
        "\n",
        "        # Hacer copia para no modificar el original\n",
        "        interpolated_sequence = landmark_sequence.copy()\n",
        "\n",
        "        # Identificar todos los landmarks que aparecen en la secuencia\n",
        "        all_landmarks = set()\n",
        "        for frame in landmark_sequence:\n",
        "            all_landmarks.update(frame.keys())\n",
        "\n",
        "        # Para cada landmark, interpolar valores faltantes\n",
        "        for landmark_name in all_landmarks:\n",
        "            # Identificar frames donde está presente\n",
        "            present_frames = {}\n",
        "            for i, frame in enumerate(landmark_sequence):\n",
        "                if landmark_name in frame:\n",
        "                    present_frames[i] = frame[landmark_name]\n",
        "\n",
        "            # Si hay gaps (al menos dos frames con el landmark pero no continuos)\n",
        "            present_indices = sorted(present_frames.keys())\n",
        "            if len(present_indices) >= 2:\n",
        "                # Interpolar para cada gap\n",
        "                for i in range(len(present_indices) - 1):\n",
        "                    start_idx = present_indices[i]\n",
        "                    end_idx = present_indices[i + 1]\n",
        "\n",
        "                    # Si hay gap (frames intermedios sin el landmark)\n",
        "                    if end_idx - start_idx > 1:\n",
        "                        start_coords = present_frames[start_idx]\n",
        "                        end_coords = present_frames[end_idx]\n",
        "\n",
        "                        # Interpolar para cada frame en el gap\n",
        "                        for j in range(start_idx + 1, end_idx):\n",
        "                            # Interpolación lineal\n",
        "                            t = (j - start_idx) / (end_idx - start_idx)\n",
        "                            interp_coords = (1 - t) * start_coords + t * end_coords\n",
        "\n",
        "                            # Actualizar frame\n",
        "                            if j < len(interpolated_sequence):\n",
        "                                interpolated_sequence[j][landmark_name] = interp_coords\n",
        "\n",
        "        return interpolated_sequence\n",
        "\n",
        "    def filter_all(self, landmark_sequence: List[Dict[str, np.ndarray]],\n",
        "                   methods: List[str] = None) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Aplica una combinación de métodos de filtrado.\"\"\"\n",
        "        try:\n",
        "            if not methods:\n",
        "                methods = ['interpolate', 'median', 'moving_average']\n",
        "\n",
        "            filtered = landmark_sequence\n",
        "\n",
        "            for method in methods:\n",
        "                if method == 'moving_average':\n",
        "                    filtered = self.moving_average(filtered)\n",
        "                elif method == 'savitzky_golay':\n",
        "                    # No implementado, usamos moving_average como fallback\n",
        "                    filtered = self.moving_average(filtered)\n",
        "                elif method == 'median':\n",
        "                    filtered = self.median_filter(filtered)\n",
        "                elif method == 'kalman':\n",
        "                    filtered = self.simple_kalman_filter(filtered)\n",
        "                elif method == 'interpolate':\n",
        "                    filtered = self.interpolate_missing_landmarks(filtered)\n",
        "\n",
        "            return filtered\n",
        "        except Exception as e:\n",
        "            print(f\"Error en filtrado: {e}\")\n",
        "            # Si hay error, devolver la secuencia original sin filtrar\n",
        "            return landmark_sequence\n",
        "\n",
        "# Función para filtrar landmarks\n",
        "def filter_landmark_sequence(landmark_frames: List[pd.DataFrame],\n",
        "                             methods: List[str] = None,\n",
        "                             window_size: int = 5) -> List[pd.DataFrame]:\n",
        "    \"\"\"Filtra una secuencia temporal de frames de landmarks.\"\"\"\n",
        "    try:\n",
        "        # Convertir DataFrames a formato interno\n",
        "        landmark_dicts = [dataframe_to_landmark_dict(frame) for frame in landmark_frames]\n",
        "\n",
        "        # Aplicar filtros\n",
        "        filter_handler = LandmarkFilter(window_size=window_size)\n",
        "        filtered_dicts = filter_handler.filter_all(landmark_dicts, methods=methods)\n",
        "\n",
        "        # Convertir de vuelta a DataFrames\n",
        "        filtered_frames = [landmark_dict_to_dataframe(frame_dict) for frame_dict in filtered_dicts]\n",
        "\n",
        "        return filtered_frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error en filter_landmark_sequence: {e}\")\n",
        "        return landmark_frames  # Devolver frames sin filtrar si hay error"
      ],
      "metadata": {
        "id": "IXthZQBSdiWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para extracción de características\n",
        "class FeatureExtractor:\n",
        "    \"\"\"\n",
        "    Clase para extraer características significativas de landmarks para clasificación de actividades.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size: int = 30):\n",
        "        self.window_size = window_size\n",
        "        # Pares de landmarks para cálculo de ángulos articulares\n",
        "        self.joint_pairs = {\n",
        "            'rodilla_izquierda': ['LEFT_HIP', 'LEFT_KNEE', 'LEFT_ANKLE'],\n",
        "            'rodilla_derecha': ['RIGHT_HIP', 'RIGHT_KNEE', 'RIGHT_ANKLE'],\n",
        "            'cadera_izquierda': ['SPINE_NAVAL', 'LEFT_HIP', 'LEFT_KNEE'],\n",
        "            'cadera_derecha': ['SPINE_NAVAL', 'RIGHT_HIP', 'RIGHT_KNEE'],\n",
        "            'hombro_izquierdo': ['NECK', 'LEFT_SHOULDER', 'LEFT_ELBOW'],\n",
        "            'hombro_derecho': ['NECK', 'RIGHT_SHOULDER', 'RIGHT_ELBOW'],\n",
        "            'codo_izquierdo': ['LEFT_SHOULDER', 'LEFT_ELBOW', 'LEFT_WRIST'],\n",
        "            'codo_derecho': ['RIGHT_SHOULDER', 'RIGHT_ELBOW', 'RIGHT_WRIST'],\n",
        "            'torso': ['NECK', 'SPINE_CHEST', 'SPINE_NAVAL']\n",
        "        }\n",
        "\n",
        "    def calculate_joint_angles(self, frame: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
        "        angles = {}\n",
        "\n",
        "        for joint_name, landmarks in self.joint_pairs.items():\n",
        "            if all(lm in frame for lm in landmarks):\n",
        "                # Extraer coordenadas 3D de los landmarks\n",
        "                p1 = frame[landmarks[0]][:3]\n",
        "                p2 = frame[landmarks[1]][:3]\n",
        "                p3 = frame[landmarks[2]][:3]\n",
        "\n",
        "                # Calcular vectores\n",
        "                v1 = p1 - p2\n",
        "                v2 = p3 - p2\n",
        "\n",
        "                # Normalizar vectores\n",
        "                v1_norm = np.linalg.norm(v1)\n",
        "                v2_norm = np.linalg.norm(v2)\n",
        "\n",
        "                if v1_norm > 0 and v2_norm > 0:\n",
        "                    # Calcular ángulo en radianes y convertir a grados\n",
        "                    cos_angle = np.clip(np.dot(v1, v2) / (v1_norm * v2_norm), -1.0, 1.0)\n",
        "                    angle_rad = np.arccos(cos_angle)\n",
        "                    angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "                    angles[joint_name] = angle_deg\n",
        "                else:\n",
        "                    angles[joint_name] = 0.0\n",
        "            else:\n",
        "                angles[joint_name] = np.nan\n",
        "\n",
        "        return angles\n",
        "\n",
        "    # Implementaciones básicas de otros métodos\n",
        "    def calculate_body_orientation(self, frame: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"Calcula la orientación del cuerpo respecto a la cámara.\"\"\"\n",
        "        # Implementación básica\n",
        "        orientation = {'angulo_giro': 0, 'inclinacion_torso': 45, 'inclinacion_lateral': 0}\n",
        "\n",
        "        # Si tenemos los landmarks necesarios, hacer un cálculo básico\n",
        "        if all(lm in frame for lm in ['LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_HIP', 'RIGHT_HIP']):\n",
        "            # Calcular vector entre hombros\n",
        "            left_shoulder = frame['LEFT_SHOULDER'][:3]\n",
        "            right_shoulder = frame['RIGHT_SHOULDER'][:3]\n",
        "            shoulder_vector = right_shoulder - left_shoulder\n",
        "\n",
        "            # Calcular vector entre caderas\n",
        "            left_hip = frame['LEFT_HIP'][:3]\n",
        "            right_hip = frame['RIGHT_HIP'][:3]\n",
        "            hip_vector = right_hip - left_hip\n",
        "\n",
        "            # Ángulo de giro simple (proyección en XZ)\n",
        "            orientation['angulo_giro'] = np.degrees(np.arctan2(shoulder_vector[0], shoulder_vector[2]))\n",
        "\n",
        "            # Inclinación del torso simple (ángulo con la vertical)\n",
        "            torso_upper = (left_shoulder + right_shoulder) / 2\n",
        "            torso_lower = (left_hip + right_hip) / 2\n",
        "            torso_vector = torso_lower - torso_upper\n",
        "            vertical = np.array([0, 1, 0])\n",
        "\n",
        "            if np.linalg.norm(torso_vector) > 0:\n",
        "                torso_norm = torso_vector / np.linalg.norm(torso_vector)\n",
        "                cos_angle_y = np.clip(np.dot(torso_norm, vertical), -1.0, 1.0)\n",
        "                orientation['inclinacion_torso'] = np.degrees(np.arccos(cos_angle_y))\n",
        "\n",
        "        return orientation\n",
        "\n",
        "    def calculate_limb_movement(self, frame_sequence: List[Dict[str, np.ndarray]], current_idx: int) -> Dict[str, float]:\n",
        "        \"\"\"Calcula características de movimiento de extremidades a partir de una secuencia.\"\"\"\n",
        "        # Implementación básica\n",
        "        movement_features = {}\n",
        "\n",
        "        limbs = ['LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_ANKLE', 'RIGHT_ANKLE']\n",
        "\n",
        "        for limb in limbs:\n",
        "            movement_features[f'{limb}_velocidad'] = 0.5\n",
        "            movement_features[f'{limb}_desplazamiento_total'] = 1.0\n",
        "\n",
        "        return movement_features\n",
        "\n",
        "    def calculate_posture_features(self, frame: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"Calcula características posturales para un frame.\"\"\"\n",
        "        # Implementación básica\n",
        "        posture_features = {\n",
        "            'altura_cadera_hombro': 0.7,\n",
        "            'simetria_piernas': 0.05,\n",
        "            'anchura_hombros': 0.4\n",
        "        }\n",
        "\n",
        "        return posture_features\n",
        "\n",
        "    def calculate_gait_features(self, frame_sequence: List[Dict[str, np.ndarray]], current_idx: int) -> Dict[str, float]:\n",
        "        \"\"\"Calcula características de la marcha para distinguir dirección (hacia/desde cámara).\"\"\"\n",
        "        # Implementación básica\n",
        "        gait_features = {\n",
        "            'frecuencia_paso': 0.8,\n",
        "            'longitud_paso': 0.6,\n",
        "            'asimetria_temporal': 0.05,\n",
        "            'velocidad_media': 1.2,\n",
        "            'direccion_movimiento': 0.1  # positivo: alejándose, negativo: acercándose\n",
        "        }\n",
        "\n",
        "        return gait_features\n",
        "\n",
        "    def extract_frame_features(self, frame: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"Extrae características estáticas de un solo frame.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Calcular ángulos articulares\n",
        "        joint_angles = self.calculate_joint_angles(frame)\n",
        "        features.update(joint_angles)\n",
        "\n",
        "        # Calcular orientación corporal\n",
        "        orientation = self.calculate_body_orientation(frame)\n",
        "        features.update(orientation)\n",
        "\n",
        "        # Calcular características posturales\n",
        "        posture = self.calculate_posture_features(frame)\n",
        "        features.update(posture)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_sequence_features(self, frame_sequence: List[Dict[str, np.ndarray]], current_idx: int) -> Dict[str, float]:\n",
        "        \"\"\"Extrae características dinámicas basadas en una secuencia de frames.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Características de movimiento de extremidades\n",
        "        limb_movement = self.calculate_limb_movement(frame_sequence, current_idx)\n",
        "        features.update(limb_movement)\n",
        "\n",
        "        # Características de marcha\n",
        "        gait = self.calculate_gait_features(frame_sequence, current_idx)\n",
        "        features.update(gait)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_all_features(self, frame_sequence: List[Dict[str, np.ndarray]], current_idx: int) -> Dict[str, float]:\n",
        "        \"\"\"Extrae todas las características (estáticas y dinámicas) para un frame.\"\"\"\n",
        "        all_features = {'frame_index': current_idx}  # Asegurar que siempre tenga frame_index\n",
        "\n",
        "        try:\n",
        "            # Extraer características estáticas del frame actual\n",
        "            if current_idx < len(frame_sequence) and frame_sequence[current_idx]:\n",
        "                static_features = self.extract_frame_features(frame_sequence[current_idx])\n",
        "                all_features.update(static_features)\n",
        "\n",
        "                # Extraer características dinámicas de la secuencia\n",
        "                dynamic_features = self.extract_sequence_features(frame_sequence, current_idx)\n",
        "                all_features.update(dynamic_features)\n",
        "\n",
        "            # Asegurarse de que las características clave estén presentes\n",
        "            key_features = ['rodilla_izquierda', 'rodilla_derecha', 'inclinacion_torso', 'altura_cadera_hombro']\n",
        "            for feature in key_features:\n",
        "                if feature not in all_features:\n",
        "                    all_features[feature] = 0.0\n",
        "\n",
        "            return all_features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en extract_all_features para frame {current_idx}: {e}\")\n",
        "            # En caso de error, devolver al menos frame_index y algunas características básicas\n",
        "            return all_features"
      ],
      "metadata": {
        "id": "C3uhB9OBdlkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para extraer características\n",
        "def extract_features_dataframe(landmark_sequence: List[pd.DataFrame], extractor=None) -> pd.DataFrame:\n",
        "    \"\"\"Extrae características de una secuencia de landmarks en formato DataFrame.\"\"\"\n",
        "    try:\n",
        "        if extractor is None:\n",
        "            extractor = FeatureExtractor()\n",
        "\n",
        "        # Convertir DataFrames a formato interno\n",
        "        landmark_dicts = []\n",
        "        for frame_df in landmark_sequence:\n",
        "            frame_dict = {}\n",
        "            for _, row in frame_df.iterrows():\n",
        "                frame_dict[row['landmark_name']] = np.array([row['x'], row['y'], row['z'], row['visibility']])\n",
        "            landmark_dicts.append(frame_dict)\n",
        "\n",
        "        # Extraer características para cada frame\n",
        "        features_list = []\n",
        "        for i in range(len(landmark_dicts)):\n",
        "            try:\n",
        "                frame_features = extractor.extract_all_features(landmark_dicts, i)\n",
        "                features_list.append(frame_features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al extraer características para frame {i}: {e}\")\n",
        "                # Añadir un conjunto básico de características en caso de error\n",
        "                features_list.append({'frame_index': i})\n",
        "\n",
        "        # Convertir a DataFrame\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en extract_features_dataframe: {e}\")\n",
        "        # Crear DataFrame vacío con al menos una columna frame_index\n",
        "        return pd.DataFrame({'frame_index': range(len(landmark_sequence))})\n",
        "\n",
        "def extract_features_from_sequence(landmark_frames: List[pd.DataFrame], window_size: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Extrae características de una secuencia de frames de landmarks.\"\"\"\n",
        "    extractor = FeatureExtractor(window_size=window_size)\n",
        "    features_df = extract_features_dataframe(landmark_frames, extractor)\n",
        "    return features_df"
      ],
      "metadata": {
        "id": "K_PagzpTeDbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para extraer características\n",
        "def extract_features_dataframe(landmark_sequence: List[pd.DataFrame], extractor=None) -> pd.DataFrame:\n",
        "    \"\"\"Extrae características de una secuencia de landmarks en formato DataFrame.\"\"\"\n",
        "    try:\n",
        "        if extractor is None:\n",
        "            extractor = FeatureExtractor()\n",
        "\n",
        "        # Convertir DataFrames a formato interno\n",
        "        landmark_dicts = []\n",
        "        for frame_df in landmark_sequence:\n",
        "            frame_dict = {}\n",
        "            for _, row in frame_df.iterrows():\n",
        "                frame_dict[row['landmark_name']] = np.array([row['x'], row['y'], row['z'], row['visibility']])\n",
        "            landmark_dicts.append(frame_dict)\n",
        "\n",
        "        # Extraer características para cada frame\n",
        "        features_list = []\n",
        "        for i in range(len(landmark_dicts)):\n",
        "            try:\n",
        "                frame_features = extractor.extract_all_features(landmark_dicts, i)\n",
        "                features_list.append(frame_features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al extraer características para frame {i}: {e}\")\n",
        "                # Añadir un conjunto básico de características en caso de error\n",
        "                features_list.append({'frame_index': i})\n",
        "\n",
        "        # Convertir a DataFrame\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en extract_features_dataframe: {e}\")\n",
        "        # Crear DataFrame vacío con al menos una columna frame_index\n",
        "        return pd.DataFrame({'frame_index': range(len(landmark_sequence))})\n",
        "\n",
        "def extract_features_from_sequence(landmark_frames: List[pd.DataFrame], window_size: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Extrae características de una secuencia de frames de landmarks.\"\"\"\n",
        "    extractor = FeatureExtractor(window_size=window_size)\n",
        "    features_df = extract_features_dataframe(landmark_frames, extractor)\n",
        "    return features_df"
      ],
      "metadata": {
        "id": "4FRxBK8ieH7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primera parte de la clase PreprocessingPipeline\n",
        "class PreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline integrado para el preprocesamiento de datos de landmarks para clasificación de actividades.\n",
        "    Combina normalización, filtrado y extracción de características.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 normalization_methods: List[str] = None,\n",
        "                 filtering_methods: List[str] = None,\n",
        "                 window_size: int = 30,\n",
        "                 feature_scaling: bool = True):\n",
        "        \"\"\"\n",
        "        Inicializa el pipeline con configuraciones específicas.\n",
        "        \"\"\"\n",
        "        # Configurar métodos de normalización\n",
        "        self.normalization_methods = normalization_methods or ['hip_center', 'body_scale', 'orientation']\n",
        "\n",
        "        # Configurar métodos de filtrado\n",
        "        self.filtering_methods = filtering_methods or ['interpolate', 'median', 'moving_average']\n",
        "\n",
        "        # Tamaño de ventana para características temporales\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Configuración para escalado de características\n",
        "        self.feature_scaling = feature_scaling\n",
        "        self.scaler = StandardScaler() if feature_scaling else None\n",
        "\n",
        "        # Inicializar componentes\n",
        "        self.normalizer = LandmarkNormalizer()\n",
        "        self.filter = LandmarkFilter(window_size=window_size)\n",
        "        self.extractor = FeatureExtractor(window_size=window_size)\n",
        "\n",
        "        # Estadísticas para monitoreo\n",
        "        self.stats = {\n",
        "            'frames_procesados': 0,\n",
        "            'frames_con_landmarks_completos': 0,\n",
        "            'frames_con_oclusiones': 0,\n",
        "            'tiempo_procesamiento': 0\n",
        "        }\n",
        "\n",
        "    def load_landmark_data(self, participant_id: str, view: str = 'frontal') -> List[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Carga datos de landmarks de un participante específico.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Construir ruta al archivo CSV de landmarks\n",
        "            landmarks_path = os.path.join(DATA_DIR, f'participante_{participant_id}/landmarks/{view}.csv')\n",
        "\n",
        "            try:\n",
        "                # Cargar CSV completo\n",
        "                all_landmarks_df = pd.read_csv(landmarks_path)\n",
        "\n",
        "                # Verificar estructura del CSV\n",
        "                required_columns = ['frame', 'landmark_name', 'x', 'y', 'z', 'visibility']\n",
        "                if not all(col in all_landmarks_df.columns for col in required_columns):\n",
        "                    raise ValueError(f\"El archivo {landmarks_path} no tiene las columnas requeridas\")\n",
        "\n",
        "                # Dividir por frames\n",
        "                frames = all_landmarks_df['frame'].unique()\n",
        "                landmark_frames = []\n",
        "\n",
        "                for frame_idx in frames:\n",
        "                    frame_data = all_landmarks_df[all_landmarks_df['frame'] == frame_idx].copy()\n",
        "                    landmark_frames.append(frame_data)\n",
        "\n",
        "                print(f\"Cargados {len(landmark_frames)} frames de landmarks para {participant_id} - vista {view}\")\n",
        "                return landmark_frames\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Archivo no encontrado: {landmarks_path}\")\n",
        "                # Crear datos de prueba si no hay archivos reales (para desarrollo)\n",
        "                return self.generate_test_data(n_frames=100)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al cargar landmarks de {participant_id}: {e}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error general en load_landmark_data: {e}\")\n",
        "            return self.generate_test_data(n_frames=100)\n",
        "\n",
        "    def generate_test_data(self, n_frames=100):\n",
        "        \"\"\"\n",
        "        Genera datos de prueba para desarrollo y debugging.\n",
        "        \"\"\"\n",
        "        print(\"Generando datos de prueba...\")\n",
        "        landmark_names = [\n",
        "            'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',\n",
        "            'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_HIP', 'RIGHT_HIP',\n",
        "            'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
        "            'NECK', 'SPINE_CHEST', 'SPINE_NAVAL'\n",
        "        ]\n",
        "\n",
        "        test_frames = []\n",
        "\n",
        "        for i in range(n_frames):\n",
        "            rows = []\n",
        "            for landmark in landmark_names:\n",
        "                # Simular coordenadas con algo de variación\n",
        "                x = np.sin(i/10) * 0.2 + np.random.normal(0, 0.01)\n",
        "                y = np.cos(i/10) * 0.2 + np.random.normal(0, 0.01)\n",
        "                z = np.sin(i/15) * 0.3 + np.random.normal(0, 0.01)\n",
        "                visibility = 0.9 + np.random.normal(0, 0.1)\n",
        "\n",
        "                rows.append({\n",
        "                    'frame': i,\n",
        "                    'landmark_name': landmark,\n",
        "                    'x': x,\n",
        "                    'y': y,\n",
        "                    'z': z,\n",
        "                    'visibility': np.clip(visibility, 0, 1)\n",
        "                })\n",
        "\n",
        "            test_frames.append(pd.DataFrame(rows))\n",
        "\n",
        "        return test_frames"
      ],
      "metadata": {
        "id": "ZsDj_CA8ekdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segunda parte de la clase PreprocessingPipeline\n",
        "def load_activity_labels(self, participant_id: str, view: str = 'frontal') -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Carga etiquetas de actividades para un participante.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construir ruta al archivo JSON de etiquetas\n",
        "        labels_path = os.path.join(DATA_DIR, f'participante_{participant_id}/etiquetas/{view}.json')\n",
        "\n",
        "        try:\n",
        "            with open(labels_path, 'r', encoding='utf-8') as f:\n",
        "                labels = json.load(f)\n",
        "\n",
        "            print(f\"Cargadas {len(labels)} etiquetas de actividades para {participant_id} - vista {view}\")\n",
        "            return labels\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Archivo de etiquetas no encontrado: {labels_path}\")\n",
        "            # Crear etiquetas de prueba si no hay archivos reales (para desarrollo)\n",
        "            return self.generate_test_labels(n_frames=100)\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar etiquetas de {participant_id}: {e}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error general en load_activity_labels: {e}\")\n",
        "        return self.generate_test_labels(n_frames=100)\n",
        "\n",
        "def generate_test_labels(self, n_frames=100):\n",
        "    \"\"\"\n",
        "    Genera etiquetas de prueba para desarrollo y debugging.\n",
        "    \"\"\"\n",
        "    print(\"Generando etiquetas de prueba...\")\n",
        "    activities = [\n",
        "        'caminar_adelante',\n",
        "        'caminar_atras',\n",
        "        'girar',\n",
        "        'sentarse',\n",
        "        'levantarse'\n",
        "    ]\n",
        "    velocities = ['lenta', 'normal', 'rapida']\n",
        "\n",
        "    # Dividir en segmentos\n",
        "    segment_size = n_frames // len(activities)\n",
        "\n",
        "    test_labels = []\n",
        "    for i, activity in enumerate(activities):\n",
        "        start_frame = i * segment_size\n",
        "        end_frame = (i + 1) * segment_size - 1 if i < len(activities) - 1 else n_frames - 1\n",
        "\n",
        "        test_labels.append({\n",
        "            'actividad': activity,\n",
        "            'frame_inicio': start_frame,\n",
        "            'frame_fin': end_frame,\n",
        "            'velocidad': np.random.choice(velocities),\n",
        "            'vista': 'frontal'\n",
        "        })\n",
        "\n",
        "    return test_labels\n",
        "\n",
        "def preprocess_landmarks(self, landmark_frames: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Aplica normalización y filtrado a los landmarks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Aplicar normalización\n",
        "        print(\"Aplicando normalización de landmarks...\")\n",
        "        normalized_frames = normalize_landmark_sequence(landmark_frames)\n",
        "\n",
        "        # Aplicar filtrado\n",
        "        print(f\"Aplicando filtros: {', '.join(self.filtering_methods)}...\")\n",
        "        filtered_frames = filter_landmark_sequence(\n",
        "            normalized_frames,\n",
        "            methods=self.filtering_methods,\n",
        "            window_size=self.window_size\n",
        "        )\n",
        "\n",
        "        # Actualizar estadísticas\n",
        "        self.stats['frames_procesados'] += len(landmark_frames)\n",
        "        self.stats['tiempo_procesamiento'] += time.time() - start_time\n",
        "\n",
        "        # Contar frames con oclusiones o landmarks incompletos\n",
        "        for frame in landmark_frames:\n",
        "            landmarks_names = frame['landmark_name'].unique()\n",
        "            if len(landmarks_names) >= 10:  # Umbral para \"completo\" (ajustado a 10 para que pase con datos de prueba)\n",
        "                self.stats['frames_con_landmarks_completos'] += 1\n",
        "            else:\n",
        "                self.stats['frames_con_oclusiones'] += 1\n",
        "\n",
        "        return filtered_frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error en preprocess_landmarks: {e}\")\n",
        "        return landmark_frames  # Devolver frames sin procesar en caso de error\n",
        "\n",
        "def extract_features(self, landmark_frames: List[pd.DataFrame]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extrae características de los landmarks procesados.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extraer características\n",
        "        print(\"Extrayendo características...\")\n",
        "        features_df = extract_features_from_sequence(\n",
        "            landmark_frames,\n",
        "            window_size=self.window_size\n",
        "        )\n",
        "\n",
        "        return features_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error en extract_features: {e}\")\n",
        "        # Crear DataFrame con al menos frame_index\n",
        "        return pd.DataFrame({'frame_index': range(len(landmark_frames))})\n",
        "\n",
        "def scale_features(self, features_df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aplica escalado de características.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not self.feature_scaling or self.scaler is None:\n",
        "            return features_df\n",
        "\n",
        "        # Crear copia para no modificar el original\n",
        "        scaled_df = features_df.copy()\n",
        "\n",
        "        # Seleccionar solo columnas numéricas\n",
        "        numeric_cols = features_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "        # Eliminar columnas que no deben ser escaladas\n",
        "        if 'frame_index' in numeric_cols:\n",
        "            numeric_cols.remove('frame_index')\n",
        "\n",
        "        # Verificar que haya columnas para escalar\n",
        "        if not numeric_cols:\n",
        "            return scaled_df\n",
        "\n",
        "        # Asegurarse de que no haya NaN\n",
        "        scaled_df[numeric_cols] = scaled_df[numeric_cols].fillna(0)\n",
        "\n",
        "        # Aplicar escalado\n",
        "        if fit:\n",
        "            scaled_values = self.scaler.fit_transform(scaled_df[numeric_cols])\n",
        "        else:\n",
        "            scaled_values = self.scaler.transform(scaled_df[numeric_cols])\n",
        "\n",
        "        scaled_df[numeric_cols] = scaled_values\n",
        "\n",
        "        return scaled_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error en scale_features: {e}\")\n",
        "        return features_df  # Devolver sin escalar si hay error\n",
        "\n",
        "# Agregar los métodos a la clase\n",
        "PreprocessingPipeline.load_activity_labels = load_activity_labels\n",
        "PreprocessingPipeline.generate_test_labels = generate_test_labels\n",
        "PreprocessingPipeline.preprocess_landmarks = preprocess_landmarks\n",
        "PreprocessingPipeline.extract_features = extract_features\n",
        "PreprocessingPipeline.scale_features = scale_features"
      ],
      "metadata": {
        "id": "iU9uknH4emrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tercera parte de la clase PreprocessingPipeline\n",
        "def assign_activity_labels(self, features_df: pd.DataFrame,\n",
        "                          activity_labels: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Asigna etiquetas de actividad a cada frame según los intervalos definidos.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Crear copia para no modificar el original\n",
        "        labeled_df = features_df.copy()\n",
        "\n",
        "        # Añadir columna de actividad con valor inicial \"desconocido\"\n",
        "        labeled_df['actividad'] = \"desconocido\"\n",
        "        labeled_df['velocidad'] = None\n",
        "        labeled_df['vista'] = None\n",
        "\n",
        "        # Asignar etiquetas según los intervalos\n",
        "        for label in activity_labels:\n",
        "            # Obtener índices dentro del intervalo\n",
        "            inicio = label.get('frame_inicio', 0)\n",
        "            fin = label.get('frame_fin', 0)\n",
        "\n",
        "            # Asignar etiqueta dentro del intervalo\n",
        "            mask = (labeled_df['frame_index'] >= inicio) & (labeled_df['frame_index'] <= fin)\n",
        "            labeled_df.loc[mask, 'actividad'] = label.get('actividad', 'desconocido')\n",
        "            labeled_df.loc[mask, 'velocidad'] = label.get('velocidad', None)\n",
        "            labeled_df.loc[mask, 'vista'] = label.get('vista', None)\n",
        "\n",
        "        return labeled_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error en assign_activity_labels: {e}\")\n",
        "        # En caso de error, añadir columnas vacías\n",
        "        if 'actividad' not in features_df.columns:\n",
        "            features_df['actividad'] = \"desconocido\"\n",
        "        if 'velocidad' not in features_df.columns:\n",
        "            features_df['velocidad'] = None\n",
        "        if 'vista' not in features_df.columns:\n",
        "            features_df['vista'] = None\n",
        "        return features_df\n",
        "\n",
        "def process_participant_data(self, participant_id: str, view: str = 'frontal') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Procesa los datos completos de un participante: carga, normaliza, filtra y extrae características.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar datos de landmarks\n",
        "        landmark_frames = self.load_landmark_data(participant_id, view)\n",
        "        if not landmark_frames:\n",
        "            print(f\"No se encontraron datos para participante {participant_id}, vista {view}\")\n",
        "            return pd.DataFrame({'frame_index': [0], 'actividad': ['desconocido']})\n",
        "\n",
        "        # Cargar etiquetas de actividades\n",
        "        activity_labels = self.load_activity_labels(participant_id, view)\n",
        "\n",
        "        # Aplicar preprocesamiento\n",
        "        processed_frames = self.preprocess_landmarks(landmark_frames)\n",
        "\n",
        "        # Extraer características\n",
        "        features_df = self.extract_features(processed_frames)\n",
        "\n",
        "        # Aplicar escalado\n",
        "        scaled_features = self.scale_features(features_df)\n",
        "\n",
        "        # Asignar etiquetas\n",
        "        if activity_labels:\n",
        "            labeled_features = self.assign_activity_labels(scaled_features, activity_labels)\n",
        "        else:\n",
        "            labeled_features = scaled_features\n",
        "            print(\"No se encontraron etiquetas de actividad\")\n",
        "            # Asegurar que hay columna de actividad\n",
        "            if 'actividad' not in labeled_features.columns:\n",
        "                labeled_features['actividad'] = \"desconocido\"\n",
        "\n",
        "        print(f\"Procesamiento completado para participante {participant_id}, vista {view}\")\n",
        "        print(f\"Estadísticas: {self.stats}\")\n",
        "\n",
        "        return labeled_features\n",
        "    except Exception as e:\n",
        "        print(f\"Error en process_participant_data: {e}\")\n",
        "        # Crear DataFrame básico con frame_index y actividad\n",
        "        return pd.DataFrame({'frame_index': [0], 'actividad': ['desconocido']})\n",
        "\n",
        "def process_all_participants(self, participant_ids: List[str],\n",
        "                            views: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Procesa los datos de múltiples participantes.\n",
        "    \"\"\"\n",
        "    if views is None:\n",
        "        views = ['frontal', 'lateral']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for participant_id in participant_ids:\n",
        "        for view in views:\n",
        "            key = f\"{participant_id}_{view}\"\n",
        "            results[key] = self.process_participant_data(participant_id, view)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Agregar los métodos a la clase\n",
        "PreprocessingPipeline.assign_activity_labels = assign_activity_labels\n",
        "PreprocessingPipeline.process_participant_data = process_participant_data\n",
        "PreprocessingPipeline.process_all_participants = process_all_participants"
      ],
      "metadata": {
        "id": "VvApBPZEepge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cuarta parte de la clase PreprocessingPipeline (visualización y guardado)\n",
        "def visualize_processing_results(self, features_df: pd.DataFrame,\n",
        "                                n_features: int = 5,\n",
        "                                show_activity_distribution: bool = True):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados del procesamiento.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if features_df.empty:\n",
        "            print(\"No hay datos para visualizar\")\n",
        "            return\n",
        "\n",
        "        # Seleccionar características numéricas\n",
        "        numeric_cols = features_df.select_dtypes(include=np.number).columns\n",
        "\n",
        "        # Eliminar columnas que no son características\n",
        "        exclude_cols = ['frame_index']\n",
        "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "        # Si no hay suficientes características, mostrar mensaje y salir\n",
        "        if len(feature_cols) < n_features:\n",
        "            print(f\"Solo hay {len(feature_cols)} características disponibles para visualizar\")\n",
        "            n_features = max(1, len(feature_cols))\n",
        "\n",
        "        # Seleccionar las primeras n características para visualizar\n",
        "        plot_features = feature_cols[:n_features]\n",
        "\n",
        "        # Crear figura\n",
        "        fig, axes = plt.subplots(len(plot_features), 1, figsize=(12, 3 * len(plot_features)))\n",
        "        if len(plot_features) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        # Graficar evolución de características\n",
        "        for i, feature in enumerate(plot_features):\n",
        "            try:\n",
        "                axes[i].plot(features_df['frame_index'], features_df[feature])\n",
        "                axes[i].set_title(f'Evolución de {feature}')\n",
        "                axes[i].grid(True)\n",
        "\n",
        "                # Si hay etiquetas de actividad, añadir sombreado por actividad\n",
        "                if 'actividad' in features_df.columns and features_df['actividad'].nunique() > 1:\n",
        "                    activities = features_df['actividad'].unique()\n",
        "                    colors = plt.cm.tab10(np.linspace(0, 1, len(activities)))\n",
        "\n",
        "                    for j, activity in enumerate(activities):\n",
        "                        activity_frames = features_df[features_df['actividad'] == activity]['frame_index']\n",
        "                        if not activity_frames.empty:\n",
        "                            for start_idx in activity_frames:\n",
        "                                if start_idx + 1 in activity_frames.values:\n",
        "                                    axes[i].axvspan(start_idx, start_idx + 1, alpha=0.3, color=colors[j])\n",
        "\n",
        "                    # Añadir leyenda\n",
        "                    from matplotlib.lines import Line2D\n",
        "                    custom_lines = [Line2D([0], [0], color=colors[i], lw=4) for i in range(len(activities))]\n",
        "                    axes[i].legend(custom_lines, activities, loc='upper right')\n",
        "            except Exception as e:\n",
        "                print(f\"Error al visualizar característica {feature}: {e}\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Mostrar distribución de actividades si está disponible\n",
        "        if show_activity_distribution and 'actividad' in features_df.columns:\n",
        "            activity_counts = features_df['actividad'].value_counts()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            activity_counts.plot(kind='bar')\n",
        "            plt.title('Distribución de Actividades')\n",
        "            plt.xlabel('Actividad')\n",
        "            plt.ylabel('Número de Frames')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Mostrar características promedio por actividad\n",
        "            if len(activity_counts) > 1:\n",
        "                # Seleccionar características más discriminativas\n",
        "                discriminative_features = [\n",
        "                    'rodilla_izquierda', 'rodilla_derecha', 'inclinacion_torso',\n",
        "                    'anchura_hombros', 'altura_cadera_hombro', 'velocidad_media'\n",
        "                ]\n",
        "\n",
        "                # Filtrar solo las que existen\n",
        "                available_features = [f for f in discriminative_features if f in features_df.columns]\n",
        "\n",
        "                if available_features:\n",
        "                    # Agrupar por actividad y calcular media\n",
        "                    grouped = features_df.groupby('actividad')[available_features].mean()\n",
        "\n",
        "                    # Visualizar\n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    grouped.plot(kind='bar')\n",
        "                    plt.title('Características Promedio por Actividad')\n",
        "                    plt.ylabel('Valor Promedio')\n",
        "                    plt.xticks(rotation=45)\n",
        "                    plt.legend(loc='best')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error en visualize_processing_results: {e}\")\n",
        "\n",
        "def save_processed_data(self, processed_data: Dict[str, pd.DataFrame], output_dir: str = None):\n",
        "    \"\"\"\n",
        "    Guarda los datos procesados en formato CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Usar directorio predeterminado si no se especifica\n",
        "        if output_dir is None:\n",
        "            output_dir = OUTPUT_DIR\n",
        "\n",
        "        # Crear directorio si no existe\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Guardar cada DataFrame\n",
        "        for key, df in processed_data.items():\n",
        "            if not df.empty:\n",
        "                output_path = os.path.join(output_dir, f\"{key}_processed.csv\")\n",
        "                df.to_csv(output_path, index=False)\n",
        "                print(f\"Datos procesados guardados en {output_path}\")\n",
        "\n",
        "        # Guardar metadatos con estadísticas\n",
        "        metadata = {\n",
        "            'stats': self.stats,\n",
        "            'configuracion': {\n",
        "                'normalization_methods': self.normalization_methods,\n",
        "                'filtering_methods': self.filtering_methods,\n",
        "                'window_size': self.window_size,\n",
        "                'feature_scaling': self.feature_scaling\n",
        "            }\n",
        "        }\n",
        "\n",
        "        metadata_path = os.path.join(output_dir, \"preprocessing_metadata.json\")\n",
        "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Metadatos de preprocesamiento guardados en {metadata_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error en save_processed_data: {e}\")\n",
        "\n",
        "def generate_processing_report(self, processed_data: Dict[str, pd.DataFrame],\n",
        "                             output_path: str = None):\n",
        "    \"\"\"\n",
        "    Genera un informe sobre el procesamiento realizado.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Usar ruta predeterminada si no se especifica\n",
        "        if output_path is None:\n",
        "            output_path = os.path.join(OUTPUT_DIR, 'preprocessing_report.md')\n",
        "\n",
        "        # Construir informe en formato Markdown\n",
        "        report = \"# Informe de Preprocesamiento de Datos\\n\\n\"\n",
        "\n",
        "        # Resumen general\n",
        "        report += \"## Resumen General\\n\\n\"\n",
        "        report += f\"- **Frames procesados**: {self.stats['frames_procesados']}\\n\"\n",
        "        report += f\"- **Frames con landmarks completos**: {self.stats['frames_con_landmarks_completos']}\\n\"\n",
        "        report += f\"- **Frames con oclusiones**: {self.stats['frames_con_oclusiones']}\\n\"\n",
        "        report += f\"- **Tiempo total de procesamiento**: {self.stats['tiempo_procesamiento']:.2f} segundos\\n\\n\"\n",
        "\n",
        "        # Configuración\n",
        "        report += \"## Configuración de Preprocesamiento\\n\\n\"\n",
        "        report += f\"- **Métodos de normalización**: {', '.join(self.normalization_methods)}\\n\"\n",
        "        report += f\"- **Métodos de filtrado**: {', '.join(self.filtering_methods)}\\n\"\n",
        "        report += f\"- **Tamaño de ventana**: {self.window_size} frames\\n\"\n",
        "        report += f\"- **Escalado de características**: {'Activado' if self.feature_scaling else 'Desactivado'}\\n\\n\"\n",
        "\n",
        "        # Detalles por participante\n",
        "        report += \"## Detalles por Participante\\n\\n\"\n",
        "\n",
        "        for key, df in processed_data.items():\n",
        "            if not df.empty:\n",
        "                report += f\"### {key}\\n\\n\"\n",
        "                report += f\"- **Frames totales**: {len(df)}\\n\"\n",
        "\n",
        "                if 'actividad' in df.columns:\n",
        "                    activity_counts = df['actividad'].value_counts()\n",
        "                    report += \"- **Distribución de actividades**:\\n\"\n",
        "                    for activity, count in activity_counts.items():\n",
        "                        report += f\"  - {activity}: {count} frames ({count/len(df)*100:.1f}%)\\n\"\n",
        "\n",
        "                report += \"\\n\"\n",
        "\n",
        "        # Conclusiones y observaciones\n",
        "        report += \"## Conclusiones y Observaciones\\n\\n\"\n",
        "        report += \"- Los datos han sido normalizados para compensar diferencias en altura y distancia a la cámara.\\n\"\n",
        "        report += \"- Se han aplicado filtros para reducir el ruido y mejorar la estabilidad de las mediciones.\\n\"\n",
        "        report += \"- Se han extraído características relevantes para la clasificación de actividades físicas.\\n\\n\"\n",
        "\n",
        "        report += \"### Calidad de los Datos\\n\\n\"\n",
        "        completeness_pct = (self.stats['frames_con_landmarks_completos'] / max(1, self.stats['frames_procesados'])) * 100\n",
        "        report += f\"- **Completitud de landmarks**: {completeness_pct:.1f}%\\n\"\n",
        "        report += f\"- **Oclusiones**: {100 - completeness_pct:.1f}%\\n\\n\"\n",
        "\n",
        "        report += \"### Siguientes Pasos\\n\\n\"\n",
        "        report += \"1. Entrenar modelos de clasificación con las características extraídas\\n\"\n",
        "        report += \"2. Evaluar rendimiento con validación cruzada\\n\"\n",
        "        report += \"3. Implementar inferencia en tiempo real\\n\"\n",
        "\n",
        "        # Guardar informe\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"Informe de preprocesamiento generado en {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error en generate_processing_report: {e}\")\n",
        "\n",
        "# Agregar los métodos a la clase\n",
        "PreprocessingPipeline.visualize_processing_results = visualize_processing_results\n",
        "PreprocessingPipeline.save_processed_data = save_processed_data\n",
        "PreprocessingPipeline.generate_processing_report = generate_processing_report"
      ],
      "metadata": {
        "id": "5IgaYdU_etVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso del Pipeline de Preprocesamiento\n",
        "try:\n",
        "    # Crear instancia del pipeline con configuración personalizada\n",
        "    pipeline = PreprocessingPipeline(\n",
        "        normalization_methods=['hip_center', 'body_scale', 'orientation'],\n",
        "        filtering_methods=['interpolate', 'median', 'moving_average'],\n",
        "        window_size=15,  # Ventana de 15 frames para características temporales (0.5 segundos a 30fps)\n",
        "        feature_scaling=True  # Aplicar escalado de características\n",
        "    )\n",
        "\n",
        "    # Procesar datos de un participante específico (o usar datos simulados si no hay reales)\n",
        "    participant_id = \"01\"  # Cambiar al ID de participante real si está disponible\n",
        "    view = \"frontal\"  # 'frontal' o 'lateral'\n",
        "\n",
        "    # Procesar datos\n",
        "    processed_data = pipeline.process_participant_data(participant_id, view)\n",
        "\n",
        "    # Mostrar información sobre los datos procesados\n",
        "    print(f\"\\nDatos procesados para participante {participant_id}, vista {view}:\")\n",
        "    print(f\"Número de frames: {len(processed_data)}\")\n",
        "    print(f\"Características extraídas: {processed_data.shape[1]}\")\n",
        "    print(f\"Columnas: {processed_data.columns.tolist()[:10]}...\")\n",
        "\n",
        "    # Visualizar resultados\n",
        "    pipeline.visualize_processing_results(processed_data, n_features=5, show_activity_distribution=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al ejecutar el pipeline: {e}\")"
      ],
      "metadata": {
        "id": "OaAzcXilev8J",
        "outputId": "db7ef31c-4896-44e0-bdfa-797eb51298af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_01/landmarks/frontal.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "\n",
            "Datos procesados para participante 01, vista frontal:\n",
            "Número de frames: 1\n",
            "Características extraídas: 2\n",
            "Columnas: ['frame_index', 'actividad']...\n",
            "Solo hay 0 características disponibles para visualizar\n",
            "Error en visualize_processing_results: Number of rows must be a positive integer, not 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x0 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesamiento de Múltiples Participantes\n",
        "try:\n",
        "    # Definir lista de participantes a procesar\n",
        "    # En un caso real, esto podría obtenerse escaneando el directorio de datos\n",
        "    participant_ids = [\"01\", \"02\", \"03\"]  # Ajustar según la disponibilidad de datos\n",
        "\n",
        "    # Procesar todos los participantes y ambas vistas\n",
        "    all_processed_data = pipeline.process_all_participants(\n",
        "        participant_ids=participant_ids,\n",
        "        views=[\"frontal\", \"lateral\"]\n",
        "    )\n",
        "\n",
        "    # Mostrar resumen de los datos procesados\n",
        "    print(\"\\nResumen de datos procesados:\")\n",
        "    for key, df in all_processed_data.items():\n",
        "        if not df.empty:\n",
        "            n_activities = df['actividad'].nunique() if 'actividad' in df.columns else 0\n",
        "            print(f\"{key}: {len(df)} frames, {n_activities} actividades\")\n",
        "\n",
        "    # Guardar datos procesados\n",
        "    pipeline.save_processed_data(all_processed_data)\n",
        "\n",
        "    # Generar informe de preprocesamiento\n",
        "    pipeline.generate_processing_report(all_processed_data)\n",
        "except Exception as e:\n",
        "    print(f\"Error al procesar múltiples participantes: {e}\")"
      ],
      "metadata": {
        "id": "n4iHDHDAeyDF",
        "outputId": "e2885b6c-ad06-4d2a-b806-f2a1facdd0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_01/landmarks/frontal.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_01/landmarks/lateral.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_02/landmarks/frontal.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_02/landmarks/lateral.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_03/landmarks/frontal.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "Archivo no encontrado: /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/dataset/participante_03/landmarks/lateral.csv\n",
            "Generando datos de prueba...\n",
            "Error en process_participant_data: 'PreprocessingPipeline' object has no attribute 'load_activity_labels'\n",
            "\n",
            "Resumen de datos procesados:\n",
            "01_frontal: 1 frames, 1 actividades\n",
            "01_lateral: 1 frames, 1 actividades\n",
            "02_frontal: 1 frames, 1 actividades\n",
            "02_lateral: 1 frames, 1 actividades\n",
            "03_frontal: 1 frames, 1 actividades\n",
            "03_lateral: 1 frames, 1 actividades\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/01_frontal_processed.csv\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/01_lateral_processed.csv\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/02_frontal_processed.csv\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/02_lateral_processed.csv\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/03_frontal_processed.csv\n",
            "Datos procesados guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/03_lateral_processed.csv\n",
            "Metadatos de preprocesamiento guardados en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/preprocessing_metadata.json\n",
            "Informe de preprocesamiento generado en /content/drive/MyDrive/ICESI/IA1/Proyecto_Anotacion_Video/processed_data/preprocessing_report.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis Exploratorio de Características\n",
        "try:\n",
        "    # Seleccionar un conjunto de datos procesados para análisis\n",
        "    if 'all_processed_data' in locals() and len(all_processed_data) > 0:\n",
        "        analysis_key = list(all_processed_data.keys())[0]\n",
        "        analysis_data = all_processed_data[analysis_key]\n",
        "\n",
        "        if 'actividad' in analysis_data.columns:\n",
        "            # 1. Análisis de correlación entre características\n",
        "            # Seleccionar características numéricas más relevantes\n",
        "            features_of_interest = [\n",
        "                'rodilla_izquierda', 'rodilla_derecha', 'inclinacion_torso',\n",
        "                'anchura_hombros', 'altura_cadera_hombro', 'direccion_movimiento',\n",
        "                'velocidad_media', 'frecuencia_paso'\n",
        "            ]\n",
        "\n",
        "            # Filtrar solo características disponibles\n",
        "            available_features = [f for f in features_of_interest if f in analysis_data.columns]\n",
        "\n",
        "            if available_features:\n",
        "                # Matriz de correlación\n",
        "                correlation = analysis_data[available_features].corr()\n",
        "\n",
        "                # Visualizar correlaciones\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
        "                plt.title(f'Correlación entre Características - {analysis_key}')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # 2. Distribución de características por actividad\n",
        "                # Seleccionar 3 características más discriminativas\n",
        "                top_features = available_features[:min(3, len(available_features))]\n",
        "\n",
        "                fig, axes = plt.subplots(len(top_features), 1, figsize=(12, 4 * len(top_features)))\n",
        "                if len(top_features) == 1:\n",
        "                    axes = [axes]\n",
        "\n",
        "                for i, feature in enumerate(top_features):\n",
        "                    sns.boxplot(x='actividad', y=feature, data=analysis_data, ax=axes[i])\n",
        "                    axes[i].set_title(f'Distribución de {feature} por Actividad')\n",
        "                    axes[i].set_xlabel('Actividad')\n",
        "                    axes[i].set_ylabel(feature)\n",
        "                    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No se encontraron características de interés en los datos procesados.\")\n",
        "        else:\n",
        "            print(\"No se encontraron etiquetas de actividad en los datos procesados.\")\n",
        "    else:\n",
        "        print(\"No hay datos procesados disponibles para análisis.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error en análisis de características: {e}\")"
      ],
      "metadata": {
        "id": "VYYo14Qse2i3",
        "outputId": "8a0bffed-9459-4bed-e5e8-8e751c07b295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se encontraron características de interés en los datos procesados.\n"
          ]
        }
      ]
    }
  ]
}